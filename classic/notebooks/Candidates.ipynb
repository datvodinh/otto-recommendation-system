{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About :** Generates candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/datvodinh/Workspace/otto-recsys/kaggle_otto_rs/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datvodinh/miniconda3/envs/kaggle-otto/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cudf\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "from pandarallel import pandarallel\n",
    "from numerize.numerize import numerize\n",
    "\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pandarallel.initialize(nb_workers=16, progress_bar=False, use_memory_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "from utils.metrics import get_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"val\":\n",
    "    REGEX = \"../output/val_parquet/*\"\n",
    "elif MODE == \"test\":\n",
    "    REGEX = \"../output/test_parquet/*\"\n",
    "elif MODE == \"extra\":\n",
    "    REGEX = \"../output/val_trimmed_parquet/*\"\n",
    "    GT_FILE = \"../output/val_labels_trimmed.parquet\"\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_CT = 50\n",
    "ITEM_CT2 = 50\n",
    "ITEM_CT3 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATRIX_FOLDER = \"../output/matrices/\"\n",
    "# MATRIX_FOLDER = \"../output/matrices_2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLICKS = True\n",
    "MULTIPLIER = 1\n",
    "\n",
    "if CLICKS:\n",
    "    SUFFIX = \"c-clicks-v7\"  # 50\n",
    "else:\n",
    "#     SUFFIX = \"c-orders-v7\"  # 50\n",
    "    SUFFIX = \"c-orders-v8\"  # 75\n",
    "#     SUFFIX = \"c-orders-v9\"  # 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.candidates_chris import (\n",
    "    load_parquets,\n",
    "    df_parallelize_run,\n",
    "    explode,\n",
    "    matrix_to_candids_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make valid lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_FOLDER = f\"../output/lists/{MODE}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = load_parquets(REGEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"extra\":  # Remove useless sessions for speed-up\n",
    "    gt = pd.read_parquet(GT_FILE)\n",
    "    kept_sessions = gt[gt[\"type\"] != \"clicks\"].drop(\"ground_truth\", axis=1)\n",
    "    kept_sessions = kept_sessions.drop_duplicates(subset=\"session\", keep=\"first\")\n",
    "\n",
    "    prev_len = len(df_val)\n",
    "    df_val: pd.DataFrame = (\n",
    "        df_val.merge(kept_sessions, on=\"session\", how=\"left\", suffixes=(\"\", \"_x\"))\n",
    "        .dropna()\n",
    "        .drop(\"type_x\", axis=1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    factor = prev_len / len(df_val)\n",
    "    print(f\"Reduced dataset size by {factor:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Popular Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clicks = (\n",
    "    df_val.loc[df_val[\"type\"] == 0, \"aid\"].value_counts().index.values[:100].tolist()\n",
    ")\n",
    "top_carts = (\n",
    "    df_val.loc[df_val[\"type\"] == 1, \"aid\"].value_counts().index.values[:100].tolist()\n",
    ")\n",
    "top_orders = (\n",
    "    df_val.loc[df_val[\"type\"] == 2, \"aid\"].value_counts().index.values[:100].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE_ = \"val\" if MODE == \"extra\" else MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_buy2buy = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_cpu-90_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20_buy2buy2 = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_cpu-99_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20_orders = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_cpu-95_{MODE_}.pqt\")\n",
    ")\n",
    "top_20_carts = top_20_orders\n",
    "\n",
    "top_20_test = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-116_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20_test2 = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-115_{MODE_}.pqt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20 = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-93_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20b = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-217_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20c = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-220_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20d = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-226_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20e = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-232_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20f = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-235_{MODE_}.pqt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_buy = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-239_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20_new = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-700_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20_new2 = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-701_{MODE_}.pqt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_40_day = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-155_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_40_day2 = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-157_{MODE_}.pqt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chris Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "\n",
    "\n",
    "def suggest_aids(df):\n",
    "    # aids=df.aid.tolist()\n",
    "    # types = df.type.tolist()\n",
    "    session = df[0]\n",
    "    aids = df[1]\n",
    "    types = df[2]\n",
    "    ds = df[4]\n",
    "    ds2 = df[6]\n",
    "\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "\n",
    "    # df2 = df.sort_values('ts',ascending=False).drop_duplicates('d')\n",
    "    # aids2 = df2.aid.tolist()\n",
    "    # unique_aids3 = list(dict.fromkeys(aids2[::-1] )) #last of each session\n",
    "    unique_aids3 = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if ds2[i] == 1][::-1])\n",
    "    )\n",
    "\n",
    "    # mx = df.d.max()\n",
    "    # aids2 = df.loc[df.d==mx].aid.tolist()\n",
    "    # unique_aids4 = list(dict.fromkeys(aids2[::-1] ))\n",
    "    mx = np.max(ds)\n",
    "    unique_aids4 = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if ds[i] == mx][::-1])\n",
    "    )\n",
    "\n",
    "    # df = df.loc[ df['type'].isin([1,2]) ]\n",
    "    # unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n",
    "    unique_buys = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if types[i] in [1, 2]][::-1])\n",
    "    )\n",
    "\n",
    "    ln = len(unique_aids)\n",
    "\n",
    "    if len(unique_aids) >= 15:\n",
    "        weights = np.logspace(0.1, 1, len(aids), base=2, endpoint=True) - 1\n",
    "        aids_temp = Counter()\n",
    "        for aid, w, t in zip(aids, weights, types):\n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20c[aid][:20] for aid in unique_aids[:2] if aid in top_20c]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.6\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20b[aid][:15] for aid in unique_aids3 if aid in top_20b]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.3\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_test2[aid][:20]\n",
    "                    for aid in unique_aids[:2]\n",
    "                    if aid in top_20_test2\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.6\n",
    "\n",
    "        result = [k for k, v in aids_temp.most_common(ITEM_CT2) if k not in unique_aids]\n",
    "\n",
    "        if len(result) < 1:\n",
    "            result += top_clicks[:1]\n",
    "        return session, result[:ITEM_CT2]\n",
    "    #         return session, (result + top_clicks[: ITEM_CT2 - len(result)])[:ITEM_CT2]\n",
    "\n",
    "    aids_temp = Counter()\n",
    "\n",
    "    weights3 = [2, 2] + [1] * 28\n",
    "    if len(unique_aids) == 1:\n",
    "        aids5 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_new2[aid][:30]\n",
    "                    for aid in unique_aids[-1:]\n",
    "                    if aid in top_20_new2\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        w5 = weights3 * int(len(aids5) // 30)\n",
    "        for aid, w in zip(aids5, w5):\n",
    "            aids_temp[aid] += w\n",
    "\n",
    "    aids2 = list(\n",
    "        itertools.chain(*[top_20[aid][:20] for aid in unique_aids if aid in top_20])\n",
    "    )\n",
    "    for i, aid in enumerate(aids2):\n",
    "        m = 0.1 + 0.9 * (ln - (i // (20))) / ln\n",
    "        aids_temp[aid] += m\n",
    "        if i % (20) == 0:\n",
    "            aids_temp[aid] += m\n",
    "\n",
    "    aids3 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20b[aid][:20] for aid in unique_aids[:2] if aid in top_20b]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids3):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (20) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "\n",
    "    aids3 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20_test2[aid][:20] for aid in unique_aids[:2] if aid in top_20_test2]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids3):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (20) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "\n",
    "    aids4 = list(\n",
    "        itertools.chain(*[top_20f[aid][:10] for aid in unique_aids4 if aid in top_20f])\n",
    "    )\n",
    "    for i, aid in enumerate(aids4):\n",
    "        w = i // (10)\n",
    "        aids_temp[aid] += 1 - w * 0.1\n",
    "        if i % (10) == 0:\n",
    "            aids_temp[aid] += 1 - w * 0.1\n",
    "\n",
    "    aids5 = list(\n",
    "        itertools.chain(*[top_20e[aid][:20] for aid in unique_aids3 if aid in top_20e])\n",
    "    )\n",
    "    for i, aid in enumerate(aids5):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (20) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "    top_aids2 = [k for k, v in aids_temp.most_common(1) if k not in unique_aids]\n",
    "\n",
    "    aids3 = list(\n",
    "        itertools.chain(*[top_20c[aid][:10] for aid in top_aids2 if aid in top_20c])\n",
    "    )\n",
    "    for i, aid in enumerate(aids3):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (10) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "    top_aids2 = [k for k, v in aids_temp.most_common(ITEM_CT2) if k not in unique_aids]\n",
    "    result = top_aids2\n",
    "\n",
    "    if len(result) < 1:\n",
    "        result += top_clicks[:1]\n",
    "    return session, result[:ITEM_CT2]\n",
    "\n",
    "\n",
    "#     return session, (result + top_clicks[: ITEM_CT2 - len(result)])[:ITEM_CT2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_clicks(df):\n",
    "    # aids=df.aid.tolist()\n",
    "    # types = df.type.tolist()\n",
    "\n",
    "    session = df[0]\n",
    "    aids = df[1]\n",
    "    types = df[2]\n",
    "    tss = df[3]\n",
    "    ds = df[4]\n",
    "    ds2 = df[6]\n",
    "    # days = df[7]\n",
    "\n",
    "    top_day = top_40_day2\n",
    "    click_aids = click_df[session][:ITEM_CT3]\n",
    "\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "\n",
    "    # df2 = df.sort_values('ts',ascending=False).drop_duplicates('d')\n",
    "    # aids2 = df2.aid.tolist()\n",
    "    # unique_aids3 = list(dict.fromkeys(aids2[::-1] )) #last of each session\n",
    "    unique_aids3 = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if ds2[i] == 1][::-1])\n",
    "    )\n",
    "\n",
    "    # mx = df.d.max()\n",
    "    # aids2 = df.loc[df.d==mx].aid.tolist()\n",
    "    # unique_aids4 = list(dict.fromkeys(aids2[::-1] ))\n",
    "    mx = np.max(ds)\n",
    "    unique_aids4 = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if ds[i] == mx][::-1])\n",
    "    )\n",
    "\n",
    "    # aids2 = df.loc[df.ts >= mx - 60*60*24].aid.tolist()\n",
    "    # unique_aids6 = list(dict.fromkeys(aids2[::-1] )) #recent 1 day\n",
    "    mx = np.max(tss)\n",
    "    unique_aids6 = list(\n",
    "        dict.fromkeys(\n",
    "            [f for i, f in enumerate(aids) if tss[i] >= mx - 60 * 60 * 24][::-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # df = df.loc[ df['type'].isin([1,2]) ]\n",
    "    # unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n",
    "    unique_buys = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if types[i] in [1, 2]][::-1])\n",
    "    )\n",
    "\n",
    "    ln = len(unique_aids)\n",
    "\n",
    "    if len(unique_aids) >= 15:\n",
    "        weights = np.logspace(0.1, 1, len(aids), base=2, endpoint=True) - 1\n",
    "        aids_temp = Counter()\n",
    "        for aid, w, t in zip(aids, weights, types):\n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20c[aid][: 20 * 2] for aid in unique_aids[:2] if aid in top_20c]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.6\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20b[aid][: 15 * 2] for aid in unique_aids3 if aid in top_20b]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.3\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_test2[aid][: 20 * 2]\n",
    "                    for aid in unique_aids[:2]\n",
    "                    if aid in top_20_test2\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.6\n",
    "\n",
    "        # aids3 = list(itertools.chain(*[top_20[aid][:10] for aid in click_aids[:5] if aid in top_20]))\n",
    "        # for i,aid in enumerate(aids3):\n",
    "        #    aids_temp[aid] += 0.3\n",
    "\n",
    "        result = [k for k, v in aids_temp.most_common(ITEM_CT)]\n",
    "        return session, (result + top_clicks[: ITEM_CT - len(result)])[:ITEM_CT]\n",
    "        # return sorted_aids\n",
    "\n",
    "    aids_temp = Counter()\n",
    "\n",
    "    # NEW\n",
    "    MM = 4\n",
    "    aids2 = list(\n",
    "        itertools.chain(\n",
    "            *[top_day[aid][: 10 * MM] for aid in unique_aids6 if aid in top_day]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids2):\n",
    "        aids_temp[aid] += 1\n",
    "\n",
    "    # NEW NEW\n",
    "    # ln0 = len(click_aids)\n",
    "    aids2 = list(\n",
    "        itertools.chain(*[top_20[aid][:20] for aid in click_aids if aid in top_20])\n",
    "    )\n",
    "    for i, aid in enumerate(aids2):\n",
    "        aids_temp[aid] += 0.5\n",
    "        # if i%20==0: aids_temp[aid] += 0.5\n",
    "\n",
    "    weights3 = [2, 2] + [1] * 28\n",
    "    if len(unique_aids) == 1:\n",
    "        aids5 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_new2[aid][:30]\n",
    "                    for aid in unique_aids[-1:]\n",
    "                    if aid in top_20_new2\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        w5 = weights3 * int(len(aids5) // 30)\n",
    "        for aid, w in zip(aids5, w5):\n",
    "            aids_temp[aid] += w\n",
    "\n",
    "    # aids2 = list(itertools.chain(*[top_20[aid][:20*2] for aid in unique_aids if aid in top_20]))\n",
    "    # for i,aid in enumerate(aids2):\n",
    "    #    m = 0.1 + 0.9*(ln-(i//(20*2)))/ln\n",
    "    #    aids_temp[aid] += m\n",
    "    #    if i%(20*2)==0: aids_temp[aid] += m\n",
    "\n",
    "    # FROM GIBA\n",
    "    for i, a in enumerate(unique_aids):\n",
    "        w0 = np.max(\n",
    "            [1 - (0.35 * i), 0.001]\n",
    "        )  # Weight aid order starting from the last one.\n",
    "        if a in top_20:\n",
    "            for j, aj in enumerate(top_20[a]):\n",
    "                w1 = np.max(\n",
    "                    [1 - (0.005 * j), 0.01]\n",
    "                )  # Weight the candidate aid from the dict\n",
    "                aids_temp[aj] += w0 * w1\n",
    "\n",
    "    aids3 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20b[aid][: 20 * 2] for aid in unique_aids[:2] if aid in top_20b]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids3):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (20 * 2) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "\n",
    "    aids3 = list(\n",
    "        itertools.chain(\n",
    "            *[\n",
    "                top_20_test2[aid][: 20 * 2]\n",
    "                for aid in unique_aids[:2]\n",
    "                if aid in top_20_test2\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids3):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (20 * 2) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "\n",
    "    # TRY GIBA HERE\n",
    "    aids4 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20f[aid][: 10 * 2] for aid in unique_aids4 if aid in top_20f]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids4):\n",
    "        w = i // (10 * 2)\n",
    "        aids_temp[aid] += 1 - w * 0.1\n",
    "        if i % (10 * 2) == 0:\n",
    "            aids_temp[aid] += 1 - w * 0.1\n",
    "\n",
    "    aids5 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20e[aid][: 20 * 2] for aid in unique_aids3 if aid in top_20e]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids5):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (20 * 2) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "    top_aids2 = [k for k, v in aids_temp.most_common(1) if k not in unique_aids]\n",
    "\n",
    "    aids3 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20c[aid][: 10 * 2] for aid in top_aids2 if aid in top_20c]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids3):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (10 * 2) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "    top_aids2 = [k for k, v in aids_temp.most_common(ITEM_CT) if k not in unique_aids]\n",
    "\n",
    "    result = unique_aids + top_aids2[: ITEM_CT - len(unique_aids)]\n",
    "    return session, (result + top_clicks[: ITEM_CT - len(result)])[:ITEM_CT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_orders(df):\n",
    "    session = df[0]\n",
    "    aids = df[1]\n",
    "    types = df[2]\n",
    "    tss = df[3]\n",
    "    ds = df[4]\n",
    "    ds1 = df[5]\n",
    "    ds2 = df[6]\n",
    "    days = df[7]\n",
    "\n",
    "    # top_day = top_40_day[ df.day.values[0] ]\n",
    "    top_day = top_40_day\n",
    "    # click_aids = click_df[df.session.values[0]][:ITEM_CT2]\n",
    "    click_aids = click_df[session][:ITEM_CT3]\n",
    "\n",
    "    # aids = df.aid.tolist()\n",
    "    # types = df.type.tolist()\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "\n",
    "    # mx = df.d.max()\n",
    "    # aids2 = df.loc[df.d==mx].aid.tolist()\n",
    "    # unique_aids4 = list(dict.fromkeys(aids2[::-1] )) # last session\n",
    "    mx = np.max(ds)\n",
    "    unique_aids4 = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if ds[i] == mx][::-1])\n",
    "    )\n",
    "\n",
    "    # mx = df.ts.max()\n",
    "    # aids2 = df.loc[df.ts >= mx - 60*60/2].aid.tolist()\n",
    "    # unique_aids5 = list(dict.fromkeys(aids2[::-1] )) #recent 1 hour\n",
    "    mx = np.max(tss)\n",
    "    unique_aids5 = list(\n",
    "        dict.fromkeys(\n",
    "            [f for i, f in enumerate(aids) if tss[i] >= mx - 60 * 60 / 2][::-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # aids2 = df.loc[df.ts >= mx - 60*60*24].aid.tolist()\n",
    "    # unique_aids6 = list(dict.fromkeys(aids2[::-1] )) #recent 1 day\n",
    "    unique_aids6 = list(\n",
    "        dict.fromkeys(\n",
    "            [f for i, f in enumerate(aids) if tss[i] >= mx - 60 * 60 * 24][::-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # df2 = df.drop_duplicates('d')\n",
    "    # aids2 = df2.aid.tolist()\n",
    "    # unique_aids2 = list(dict.fromkeys(aids2[::-1] )) #first of each session\n",
    "    unique_aids2 = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if ds1[i] == 1][::-1])\n",
    "    )\n",
    "\n",
    "    # df2 = df.sort_values('ts',ascending=False).drop_duplicates('d')\n",
    "    # aids2 = df2.aid.tolist()\n",
    "    # unique_aids3 = list(dict.fromkeys(aids2 )) #last of each session\n",
    "    unique_aids3 = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if ds2[i] == 1][::-1])\n",
    "    )\n",
    "\n",
    "    # df = df.loc[ df['type'].isin([1,2]) ]\n",
    "    # unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n",
    "    unique_buys = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if types[i] in [1, 2]][::-1])\n",
    "    )\n",
    "\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights = np.logspace(0.5, 1, len(aids), base=2, endpoint=True) - 1\n",
    "        aids_temp = Counter()\n",
    "        for aid, w, t in zip(aids, weights, types):\n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        for aid in unique_aids2:\n",
    "            aids_temp[aid] += 0.5\n",
    "        for aid in unique_aids3:\n",
    "            aids_temp[aid] += 0.5\n",
    "\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_buy2buy[aid][:40]\n",
    "                    for aid in unique_buys\n",
    "                    if aid in top_20_buy2buy\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 40 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_buy2buy2[aid][:40]\n",
    "                    for aid in unique_buys\n",
    "                    if aid in top_20_buy2buy2\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.1\n",
    "            if i % 40 == 0:\n",
    "                aids_temp[aid] += 0.1\n",
    "\n",
    "        aids4 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20_test[aid][:40] for aid in unique_aids if aid in top_20_test]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids4):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 40 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "        aids5 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20c[aid][:20] for aid in unique_aids[:1] if aid in top_20c]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids5):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 20 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "        aids6 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20d[aid][:20] for aid in unique_buys[:1] if aid in top_20d]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids6):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 20 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "\n",
    "        aids7 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20b[aid][:5] for aid in unique_aids3 if aid in top_20b]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids7):\n",
    "            aids_temp[aid] += 0.25\n",
    "            if i % 5 == 0:\n",
    "                aids_temp[aid] += 0.25\n",
    "        aids7 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20b[aid][:5] for aid in unique_aids2 if aid in top_20b]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids7):\n",
    "            aids_temp[aid] += 0.125\n",
    "            if i % 5 == 0:\n",
    "                aids_temp[aid] += 0.125\n",
    "\n",
    "        # NEW STUFF\n",
    "        aids4 = list(\n",
    "            itertools.chain(\n",
    "                *[top_day[aid][:40] for aid in unique_aids6 if aid in top_day]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids4):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 40 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "        aids4 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20_test[aid][:20] for aid in click_aids if aid in top_20_test]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids4):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 20 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "        aids4 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20_buy[aid][:20] for aid in click_aids if aid in top_20_buy]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids4):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 20 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "        for aid in click_aids:\n",
    "            aids_temp[aid] += 0.05\n",
    "\n",
    "        result = [k for k, v in aids_temp.most_common(ITEM_CT)]\n",
    "\n",
    "        if MODE != \"test\":\n",
    "            if len(result) < 1:\n",
    "                result += top_orders[:1]\n",
    "            return session, result[:ITEM_CT]\n",
    "        else:\n",
    "            return session, (result + top_orders[: ITEM_CT - len(result)])[:ITEM_CT]\n",
    "\n",
    "    #         return session, (result + top_orders[: ITEM_CT - len(result)])[:ITEM_CT]\n",
    "\n",
    "    weights = [2, 2] + [1] * 8  # + [0]*30\n",
    "    weights2 = [2, 2] + [1] * 53  # + [0]*25\n",
    "    weights3 = [2, 2] + [1] * 18  # + [0]*70\n",
    "    weights4 = [2, 2] + [1] * 38  # + [0]*70\n",
    "    weights5 = [2, 2] + [1] * 28  # + [0]*70\n",
    "\n",
    "    ln = len(unique_aids)\n",
    "\n",
    "    MM = 3\n",
    "    aids_temp = Counter()\n",
    "    aids2 = list(\n",
    "        itertools.chain(\n",
    "            *[\n",
    "                top_20_orders[aid][: 10 * MM]\n",
    "                for aid in unique_aids\n",
    "                if aid in top_20_orders\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    w2 = weights5 * int(len(aids2) // (10 * MM))\n",
    "    aids3 = list(\n",
    "        itertools.chain(\n",
    "            *[\n",
    "                top_20_buy2buy[aid][: 10 * MM]\n",
    "                for aid in unique_buys\n",
    "                if aid in top_20_buy2buy\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    w3 = weights5 * int(len(aids3) // (10 * MM))\n",
    "    aids4 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20_test[aid][: 10 * MM] for aid in unique_aids if aid in top_20_test]\n",
    "        )\n",
    "    )\n",
    "    w4 = weights5 * int(len(aids4) // (10 * MM))\n",
    "    aids5 = list(\n",
    "        itertools.chain(\n",
    "            *[\n",
    "                top_20_buy2buy2[aid][: 10 * MM]\n",
    "                for aid in unique_buys\n",
    "                if aid in top_20_buy2buy2\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    w5 = weights5 * int(len(aids5) // (10 * MM))\n",
    "    for i, (aid, w) in enumerate(zip(aids2, w2)):\n",
    "        m = 0.25 + 0.75 * (ln - (i // (10 * MM))) / ln\n",
    "        aids_temp[aid] += w * m\n",
    "    for i, (aid, w) in enumerate(zip(aids3, w3)):\n",
    "        aids_temp[aid] += w / 2\n",
    "    for i, (aid, w) in enumerate(zip(aids4, w4)):\n",
    "        m = 0.25 + 0.75 * (ln - (i // (10 * MM))) / ln\n",
    "        aids_temp[aid] += w * m\n",
    "    for i, (aid, w) in enumerate(zip(aids5, w5)):\n",
    "        aids_temp[aid] += w / 2\n",
    "\n",
    "    # NEW\n",
    "    MM = 4\n",
    "    aids2 = list(\n",
    "        itertools.chain(\n",
    "            *[top_day[aid][: 10 * MM] for aid in unique_aids6 if aid in top_day]\n",
    "        )\n",
    "    )\n",
    "    w2 = weights4 * int(len(aids2) // (10 * MM))\n",
    "    for i, (aid, w) in enumerate(zip(aids2, w2)):\n",
    "        m = 0.25 + 0.75 * (ln - (i // (10 * MM))) / ln\n",
    "        aids_temp[aid] += 1  # w*m\n",
    "\n",
    "    # NEW\n",
    "    ln0 = len(click_aids)\n",
    "    aids4 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20_test[aid][:20] for aid in click_aids if aid in top_20_test]\n",
    "        )\n",
    "    )\n",
    "    w4 = weights3 * int(len(aids4) // (20))\n",
    "    for i, (aid, w) in enumerate(zip(aids4, w4)):\n",
    "        m = 0.25 + 0.75 * (ln0 - (i // (20))) / ln0\n",
    "        aids_temp[aid] += w * m\n",
    "    aids4 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20_buy[aid][:20] for aid in click_aids if aid in top_20_buy]\n",
    "        )\n",
    "    )\n",
    "    w4 = weights3 * int(len(aids4) // (20))\n",
    "    for i, (aid, w) in enumerate(zip(aids4, w4)):\n",
    "        m = 0.25 + 0.75 * (ln0 - (i // (20))) / ln0\n",
    "        aids_temp[aid] += w * m\n",
    "    for aid in click_aids:\n",
    "        aids_temp[aid] += 1\n",
    "\n",
    "    aids5 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20c[aid][:55] for aid in unique_aids[:1] if aid in top_20c]\n",
    "        )\n",
    "    )\n",
    "    w5 = weights2 * int(len(aids5) // 55)\n",
    "    for aid, w in zip(aids5, w5):\n",
    "        aids_temp[aid] += w\n",
    "\n",
    "    if len(unique_aids) == 1:\n",
    "        aids5 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_new2[aid][:20]\n",
    "                    for aid in unique_aids[-1:]\n",
    "                    if aid in top_20_new2\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        w5 = weights3 * int(len(aids5) // 20)\n",
    "        for aid, w in zip(aids5, w5):\n",
    "            aids_temp[aid] += w\n",
    "        aids5 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20_new[aid][:20] for aid in unique_aids[-1:] if aid in top_20_new]\n",
    "            )\n",
    "        )\n",
    "        w5 = weights3 * int(len(aids5) // 20)\n",
    "        for aid, w in zip(aids5, w5):\n",
    "            aids_temp[aid] += w\n",
    "\n",
    "    aids5 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20d[aid][:20] for aid in unique_buys[:1] if aid in top_20d]\n",
    "        )\n",
    "    )\n",
    "    w5 = weights3 * int(len(aids5) // 20)\n",
    "    for aid, w in zip(aids5, w5):\n",
    "        aids_temp[aid] += w\n",
    "\n",
    "    ln2 = len(unique_aids5)\n",
    "    aids5 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20_buy[aid][:20] for aid in unique_aids5 if aid in top_20_buy]\n",
    "        )\n",
    "    )\n",
    "    w5 = weights3 * int(len(aids5) // 20)\n",
    "    for aid, w in zip(aids5, w5):\n",
    "        aids_temp[aid] += 2 * w / ln2\n",
    "\n",
    "    aids4 = list(\n",
    "        itertools.chain(*[top_20f[aid][:5] for aid in unique_aids4 if aid in top_20f])\n",
    "    )\n",
    "    for i, aid in enumerate(aids4):\n",
    "        w = i // 5\n",
    "        aids_temp[aid] += 1 / 2 - w * 0.05\n",
    "        if i % 5 == 0:\n",
    "            aids_temp[aid] += 1 / 2 - w * 0.05\n",
    "    aids5 = list(\n",
    "        itertools.chain(*[top_20e[aid][:55] for aid in unique_aids3 if aid in top_20e])\n",
    "    )\n",
    "    w5 = weights2 * int(len(aids5) // 55)\n",
    "    for i, (aid, w) in enumerate(zip(aids5, w5)):\n",
    "        w2 = i // 55\n",
    "        aids_temp[aid] += w - w2 * 0.1\n",
    "    aids5 = list(\n",
    "        itertools.chain(*[top_20e[aid][:10] for aid in unique_aids2 if aid in top_20e])\n",
    "    )\n",
    "    w5 = weights * int(len(aids5) // 10)\n",
    "    for i, (aid, w) in enumerate(zip(aids5, w5)):\n",
    "        w2 = i // 10\n",
    "        aids_temp[aid] += w / 2.0 - w2 * 0.05\n",
    "\n",
    "    sorted_aids = [k for k, v in aids_temp.most_common(ITEM_CT) if k not in unique_aids]\n",
    "    result = unique_aids + sorted_aids[: ITEM_CT - len(unique_aids)]\n",
    "\n",
    "    if MODE != \"test\":\n",
    "        if len(result) < 1:\n",
    "            result += top_orders[:1]\n",
    "        return session, result[:ITEM_CT]\n",
    "    else:\n",
    "        return session, (result + top_orders[: ITEM_CT - len(result)])[:ITEM_CT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../output/lists/val/group_0.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle-otto/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../output/lists/val/group_0.pkl'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "PIECES = 10\n",
    "\n",
    "valid_bysession_list = []\n",
    "for PART in range(PIECES):\n",
    "    with open(LIST_FOLDER + f\"group_{PART}.pkl\", 'rb') as f:\n",
    "        valid_bysession_list.extend(pickle.load(f))\n",
    "    \n",
    "print(len(valid_bysession_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of processes must be at least 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[0;32m~/Workspace/otto-recsys/kaggle_otto_rs/src/data/candidates_chris.py:20\u001b[0m, in \u001b[0;36mdf_parallelize_run\u001b[0;34m(func, t_split)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mPool multiprocessing for speedup.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    pandas DataFrame: Results.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m num_cores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin([\u001b[38;5;241m20\u001b[39m, \u001b[38;5;28mlen\u001b[39m(t_split)])\n\u001b[0;32m---> 20\u001b[0m pool \u001b[38;5;241m=\u001b[39m Pool(num_cores)\n\u001b[1;32m     21\u001b[0m df \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mmap(func, t_split)\n\u001b[1;32m     22\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle-otto/lib/python3.11/multiprocessing/context.py:119\u001b[0m, in \u001b[0;36mBaseContext.Pool\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Returns a process pool object'''\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pool\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pool(processes, initializer, initargs, maxtasksperchild,\n\u001b[1;32m    120\u001b[0m             context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_context())\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle-otto/lib/python3.11/multiprocessing/pool.py:205\u001b[0m, in \u001b[0;36mPool.__init__\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild, context)\u001b[0m\n\u001b[1;32m    203\u001b[0m     processes \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mcpu_count() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of processes must be at least 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxtasksperchild \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maxtasksperchild, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m maxtasksperchild \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Number of processes must be at least 1"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists(f\"../output/candidates/clicks_fast_{MODE}.parquet\"):\n",
    "    preds = df_parallelize_run(suggest_aids, valid_bysession_list)\n",
    "    \n",
    "    pred_df = cudf.DataFrame(\n",
    "        cudf.Series([f[1] for f in preds], index=[f[0] for f in preds])\n",
    "    ).reset_index()\n",
    "    pred_df.columns = [\"session\", \"candidates\"]\n",
    "\n",
    "    pred_df.to_parquet(f\"../output/candidates/clicks_fast_{MODE}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/home/datvodinh/Workspace/otto-recsys/kaggle_otto_rs/src/../output/candidates/clicks_fast_val.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m click_df \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m     cudf\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../output/candidates/clicks_fast_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidates\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle-otto/lib/python3.11/site-packages/cudf/utils/performance_tracking.py:51\u001b[0m, in \u001b[0;36m_performance_tracking.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nvtx\u001b[38;5;241m.\u001b[39menabled():\n\u001b[1;32m     44\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m     45\u001b[0m         nvtx\u001b[38;5;241m.\u001b[39mannotate(\n\u001b[1;32m     46\u001b[0m             message\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m         )\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle-otto/lib/python3.11/site-packages/cudf/io/parquet.py:578\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(filepath_or_buffer, engine, columns, storage_options, filesystem, filters, row_groups, use_pandas_metadata, categorical_partitions, bytes_per_thread, dataset_kwargs, nrows, skip_rows, allow_mismatched_pq_schemas, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m partition_categories \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fs \u001b[38;5;129;01mand\u001b[39;00m paths:\n\u001b[1;32m    573\u001b[0m     (\n\u001b[1;32m    574\u001b[0m         paths,\n\u001b[1;32m    575\u001b[0m         row_groups,\n\u001b[1;32m    576\u001b[0m         partition_keys,\n\u001b[1;32m    577\u001b[0m         partition_categories,\n\u001b[0;32m--> 578\u001b[0m     ) \u001b[38;5;241m=\u001b[39m _process_dataset(\n\u001b[1;32m    579\u001b[0m         paths\u001b[38;5;241m=\u001b[39mpaths,\n\u001b[1;32m    580\u001b[0m         fs\u001b[38;5;241m=\u001b[39mfs,\n\u001b[1;32m    581\u001b[0m         filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[1;32m    582\u001b[0m         row_groups\u001b[38;5;241m=\u001b[39mrow_groups,\n\u001b[1;32m    583\u001b[0m         categorical_partitions\u001b[38;5;241m=\u001b[39mcategorical_partitions,\n\u001b[1;32m    584\u001b[0m         dataset_kwargs\u001b[38;5;241m=\u001b[39mdataset_kwargs,\n\u001b[1;32m    585\u001b[0m     )\n\u001b[1;32m    586\u001b[0m filepath_or_buffer \u001b[38;5;241m=\u001b[39m paths \u001b[38;5;28;01mif\u001b[39;00m paths \u001b[38;5;28;01melse\u001b[39;00m filepath_or_buffer\n\u001b[1;32m    588\u001b[0m \u001b[38;5;66;03m# Prepare remote-IO options\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle-otto/lib/python3.11/site-packages/cudf/utils/performance_tracking.py:51\u001b[0m, in \u001b[0;36m_performance_tracking.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nvtx\u001b[38;5;241m.\u001b[39menabled():\n\u001b[1;32m     44\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m     45\u001b[0m         nvtx\u001b[38;5;241m.\u001b[39mannotate(\n\u001b[1;32m     46\u001b[0m             message\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m         )\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle-otto/lib/python3.11/site-packages/cudf/io/parquet.py:389\u001b[0m, in \u001b[0;36m_process_dataset\u001b[0;34m(paths, fs, filters, row_groups, categorical_partitions, dataset_kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m     filters \u001b[38;5;241m=\u001b[39m filters_to_expression(filters)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# Initialize ds.FilesystemDataset\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# TODO: Remove the if len(paths) workaround after following bug is fixed:\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# https://issues.apache.org/jira/browse/ARROW-16438\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mdataset(\n\u001b[1;32m    390\u001b[0m     source\u001b[38;5;241m=\u001b[39mpaths[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(paths) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m paths,\n\u001b[1;32m    391\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfs,\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\n\u001b[1;32m    393\u001b[0m         dataset_kwargs\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m {\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    396\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartitioning\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    397\u001b[0m         }\n\u001b[1;32m    398\u001b[0m     ),\n\u001b[1;32m    399\u001b[0m )\n\u001b[1;32m    401\u001b[0m file_list \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mfiles\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(file_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle-otto/lib/python3.11/site-packages/pyarrow/dataset.py:794\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    783\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    784\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[1;32m    785\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mignore_prefixes\n\u001b[1;32m    791\u001b[0m )\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[0;32m--> 794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, FileInfo) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle-otto/lib/python3.11/site-packages/pyarrow/dataset.py:476\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    474\u001b[0m         fs, paths_or_selector \u001b[38;5;241m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 476\u001b[0m     fs, paths_or_selector \u001b[38;5;241m=\u001b[39m _ensure_single_source(source, filesystem)\n\u001b[1;32m    478\u001b[0m options \u001b[38;5;241m=\u001b[39m FileSystemFactoryOptions(\n\u001b[1;32m    479\u001b[0m     partitioning\u001b[38;5;241m=\u001b[39mpartitioning,\n\u001b[1;32m    480\u001b[0m     partition_base_dir\u001b[38;5;241m=\u001b[39mpartition_base_dir,\n\u001b[1;32m    481\u001b[0m     exclude_invalid_files\u001b[38;5;241m=\u001b[39mexclude_invalid_files,\n\u001b[1;32m    482\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mselector_ignore_prefixes\n\u001b[1;32m    483\u001b[0m )\n\u001b[1;32m    484\u001b[0m factory \u001b[38;5;241m=\u001b[39m FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle-otto/lib/python3.11/site-packages/pyarrow/dataset.py:441\u001b[0m, in \u001b[0;36m_ensure_single_source\u001b[0;34m(path, filesystem)\u001b[0m\n\u001b[1;32m    439\u001b[0m     paths_or_selector \u001b[38;5;241m=\u001b[39m [path]\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /home/datvodinh/Workspace/otto-recsys/kaggle_otto_rs/src/../output/candidates/clicks_fast_val.parquet"
     ]
    }
   ],
   "source": [
    "click_df = (\n",
    "    cudf.read_parquet(f\"../output/candidates/clicks_fast_{MODE}.parquet\")\n",
    "    .set_index(\"session\")[\"candidates\"]\n",
    "    .to_pandas()\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if CLICKS:\n",
    "    preds = df_parallelize_run(suggest_clicks, valid_bysession_list)\n",
    "    pred_df = pd.Series([f[1] for f in preds], index=[f[0] for f in preds])\n",
    "else:\n",
    "    preds = df_parallelize_run(suggest_orders, valid_bysession_list)\n",
    "    pred_df = pd.Series([f[1] for f in preds], index=[f[0] for f in preds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_pred_df = pd.DataFrame(\n",
    "    pred_df.add_suffix(\"_clicks\"), columns=[\"labels\"]\n",
    ").reset_index()\n",
    "orders_pred_df = pd.DataFrame(\n",
    "    pred_df.add_suffix(\"_orders\"), columns=[\"labels\"]\n",
    ").reset_index()\n",
    "carts_pred_df = pd.DataFrame(\n",
    "    pred_df.add_suffix(\"_carts\"), columns=[\"labels\"]\n",
    ").reset_index()\n",
    "\n",
    "pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df], ignore_index=True)\n",
    "pred_df.columns = [\"session_type\", \"labels_l\"]\n",
    "pred_df[\"labels\"] = pred_df[\"labels_l\"].apply(lambda x: \" \".join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE != \"test\":\n",
    "    gt = pd.read_parquet(GT_FILE)\n",
    "\n",
    "    recs = []\n",
    "    df_pred = pred_df[[\"session_type\", \"labels_l\"]].copy()\n",
    "    df_pred.columns = [\"session_type\", \"candidates\"]\n",
    "    df_pred[\"session\"] = (\n",
    "        df_pred[\"session_type\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "    )\n",
    "    df_pred[\"type\"] = df_pred[\"session_type\"].apply(lambda x: x.split(\"_\")[1])\n",
    "\n",
    "    df_pred = df_pred.merge(gt, on=[\"session\", \"type\"], how=\"left\")\n",
    "\n",
    "    for col in CLASSES:\n",
    "        df_pred_c = df_pred[df_pred[\"type\"] == col]\n",
    "\n",
    "        n_preds, n_gts, n_found = get_coverage(\n",
    "            df_pred_c[\"candidates\"].values, df_pred_c[\"ground_truth\"].values\n",
    "        )\n",
    "        print(\n",
    "            f\"- {col} \\t- Found {numerize(n_found)} GTs with {numerize(n_preds)} candidates (pos_prop={n_found / n_preds * 100 :.2f}%)\\t-  Highest reachable Recall : {n_found / n_gts :.4f}\"\n",
    "        )\n",
    "\n",
    "        recs.append(n_found / n_gts)\n",
    "\n",
    "    cv = np.average(recs, weights=WEIGHTS)\n",
    "    print(f\"\\n-> CV : {cv:.4f}\")\n",
    "\n",
    "    del clicks_pred_df, orders_pred_df, carts_pred_df, df_pred\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 75\n",
    " - clicks \t- Found 1.11M GTs with 134.98M candidates (pos_prop=0.82%)\t-  Highest reachable Recall : 0.6312\n",
    " - carts \t- Found 293.95K GTs with 134.98M candidates (pos_prop=0.22%)\t-  Highest reachable Recall : 0.5103\n",
    " - orders \t- Found 222.1K GTs with 134.98M candidates (pos_prop=0.16%)\t-  Highest reachable Recall : 0.7090\n",
    "- 50\n",
    " - clicks \t- Found 1.05M GTs with 90.01M candidates (pos_prop=1.17%)\t-  Highest reachable Recall : 0.5999\n",
    " - carts \t- Found 279.92K GTs with 90.01M candidates (pos_prop=0.31%)\t-  Highest reachable Recall : 0.4859\n",
    " - orders \t- Found 217.58K GTs with 90.01M candidates (pos_prop=0.24%)\t-  Highest reachable Recall : 0.6946\n",
    "- 50 clicks\n",
    " - clicks \t- Found 1.09M GTs with 90.06M candidates (pos_prop=1.21%)\t-  Highest reachable Recall : 0.6224\n",
    " - carts \t- Found 272.36K GTs with 90.06M candidates (pos_prop=0.30%)\t-  Highest reachable Recall : 0.4728\n",
    " - orders \t- Found 211.46K GTs with 90.06M candidates (pos_prop=0.23%)\t-  Highest reachable Recall : 0.6751"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_candids = pred_df[[\"session_type\", \"labels_l\"]].copy()\n",
    "df_candids.columns = [\"session\", \"candidates\"]\n",
    "df_candids[\"session\"] = (\n",
    "    df_candids[\"session\"].apply(lambda x: x.split(\"_\")[0]).astype(\"int32\")\n",
    ")\n",
    "df_candids = df_candids.drop_duplicates(keep=\"first\", subset=\"session\").reset_index(\n",
    "    drop=True\n",
    ")\n",
    "df_candids = df_candids.sort_values(\"session\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE != \"test\":\n",
    "    gt = pd.read_parquet(GT_FILE)\n",
    "    gt[\"ground_truth\"] = gt[\"ground_truth\"].apply(lambda x: x.tolist())\n",
    "\n",
    "    for col in CLASSES:\n",
    "        if f\"gt_{col}\" not in df_candids.columns:\n",
    "            df_candids = df_candids.merge(\n",
    "                gt[gt[\"type\"] == col].drop(\"type\", axis=1), how=\"left\"\n",
    "            ).rename(columns={\"ground_truth\": f\"gt_{col}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_candids = explode(df_candids, test=(MODE == \"test\"))\n",
    "\n",
    "df_candids.to_parquet(\n",
    "    f\"../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\", index=False\n",
    ")\n",
    "print(f\"Saved to ../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theo's version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.candidates import (\n",
    "    load_parquets,\n",
    "    create_candidates,\n",
    "    explode,\n",
    "    matrix_to_candids_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"extra\"\n",
    "SUFFIX = \"v5\"  # 6 for new matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MATRIX = 20\n",
    "MAX_COOC = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"val\":\n",
    "    PARQUET_FILES = \"../output/val_parquet/*\"\n",
    "elif MODE == \"test\":\n",
    "    PARQUET_FILES = \"../output/test_parquet/*\"\n",
    "elif MODE == \"extra\":\n",
    "    PARQUET_FILES =  \"../output/val_trimmed_parquet/*\"\n",
    "    GT_FILE = \"../output/val_labels_trimmed.parquet\"\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "MATRIX_FOLDER = \"../output/matrices/\"\n",
    "# MATRIX_FOLDER = \"../output/matrices_2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_parquets(PARQUET_FILES)\n",
    "df = df.sort_values([\"session\", \"ts\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"extra\":  # Remove useless sessions for speed-up\n",
    "    gt = pd.read_parquet(GT_FILE)\n",
    "    kept_sessions = gt[gt['type'] != \"clicks\"].drop('ground_truth', axis=1)\n",
    "    kept_sessions = kept_sessions.drop_duplicates(subset=\"session\", keep=\"first\")\n",
    "\n",
    "    prev_len = len(df)\n",
    "    df = df.merge(\n",
    "        kept_sessions, on=\"session\", how=\"left\", suffixes=('', '_x')\n",
    "    ).dropna(0).drop('type_x', axis=1).reset_index(drop=True)\n",
    "\n",
    "    factor = prev_len / len(df)\n",
    "    print(f'Reduced dataset size by {factor:.1f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE_ = \"val\" if MODE == \"extra\" else MODE\n",
    "    \n",
    "clicks_candids = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_123_temporal_{N_MATRIX}_{MODE_}.pqt\")\n",
    ")\n",
    "type_weighted_candids = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(\n",
    "        MATRIX_FOLDER + f\"matrix_123_type0.590.5_{N_MATRIX}_{MODE_}.pqt\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = create_candidates(df, clicks_candids, type_weighted_candids, max_cooc=MAX_COOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE != \"test\":\n",
    "    recalls = []\n",
    "    gt = pd.read_parquet(GT_FILE)\n",
    "\n",
    "    for col in CLASSES:\n",
    "        if f\"gt_{col}\" not in df.columns:\n",
    "            df = df.merge(\n",
    "                gt[gt[\"type\"] == col].drop(\"type\", axis=1), how=\"left\"\n",
    "            ).rename(columns={\"ground_truth\": f\"gt_{col}\"})\n",
    "\n",
    "        n_preds, n_gts, n_found = get_coverage(\n",
    "            df[\"candidates\"].values, df[f\"gt_{col}\"].values\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"- {col} \\t- Found {numerize(n_found)} GTs with {numerize(n_preds)} candidates (pos_prop={n_found / n_preds * 100 :.2f}%)\\t-  Highest reachable Recall : {n_found / n_gts :.4f}\"\n",
    "        )\n",
    "        recalls.append(n_found / n_gts)\n",
    "\n",
    "    cv = np.average(recalls, weights=WEIGHTS)\n",
    "    print(f\"\\n-> Highest reachable CV : {cv:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- clicks \t- Found 1.02M GTs with 89.04M candidates (pos_prop=1.14%)\t-  Highest reachable Recall : 0.5806\n",
    "- carts \t- Found 277.39K GTs with 89.04M candidates (pos_prop=0.31%)\t-  Highest reachable Recall : 0.4816\n",
    "- orders \t- Found 217.5K GTs with 89.04M candidates (pos_prop=0.24%)\t-  Highest reachable Recall : 0.6944\n",
    "\n",
    "-> Highest reachable CV : 0.619"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode & saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = explode(df, test=(MODE == \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(f\"../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\", index=False)\n",
    "print(f\"Saved to ../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blend Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"extra\"  # \"test\"  \"extra\"\n",
    "CLICKS = False\n",
    "\n",
    "SUFFIX = \"cv7+-tv5\"\n",
    "\n",
    "if CLICKS:\n",
    "    SUFFIX = \"clicks_\" + SUFFIX\n",
    "\n",
    "if MODE == \"extra\":\n",
    "    GT_FILE = \"../output/val_labels_trimmed.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chris_candids_clicks = cudf.read_parquet(\n",
    "    f\"../output/candidates/candidates_c-clicks-v7_{MODE}.parquet\"  # TODO : v7\n",
    ")\n",
    "\n",
    "chris_candids = cudf.read_parquet(\n",
    "    f\"../output/candidates/candidates_c-orders-v7_{MODE}.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theo_candids = cudf.read_parquet(f\"../output/candidates/candidates_v5_{MODE}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candids = (\n",
    "    cudf.concat([\n",
    "        chris_candids_clicks,  # comment for cv7-tv5\n",
    "        chris_candids, \n",
    "        theo_candids,\n",
    "    ])\n",
    "    .drop_duplicates(keep=\"first\", subset=[\"session\", \"candidates\"])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfg = candids.groupby('session').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candids.to_parquet(\n",
    "    f\"../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\", index=False\n",
    ")\n",
    "print(f\"Saved to ../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE != \"test\":\n",
    "    df = candids[[\"session\", \"candidates\"]].groupby(\"session\").agg(list)\n",
    "    df = df.reset_index().to_pandas()\n",
    "\n",
    "    GT_FILE = '../output/val_labels.parquet' if MODE == \"val\" else '../output/val_labels_trimmed.parquet'\n",
    "    gt = pd.read_parquet(GT_FILE)\n",
    "\n",
    "    recalls = []\n",
    "    for col in CLASSES:\n",
    "        if f\"gt_{col}\" not in df.columns:\n",
    "            df = df.merge(\n",
    "                gt[gt[\"type\"] == col].drop(\"type\", axis=1), how=\"left\"\n",
    "            ).rename(columns={\"ground_truth\": f\"gt_{col}\"})\n",
    "\n",
    "        n_preds, n_gts, n_found = get_coverage(\n",
    "            df[\"candidates\"].values, df[f\"gt_{col}\"].values\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"- {col} \\t- Found {numerize(n_found)} GTs with {numerize(n_preds)} candidates (pos_prop={n_found / n_preds * 100 :.2f}%)\\t-  Highest reachable Recall : {n_found / n_gts :.4f}\"\n",
    "        )\n",
    "        recalls.append(n_found / n_gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chris v7 clicks + Theo v5**\n",
    "- clicks \t- Found 1.12M GTs with 128.21M candidates (pos_prop=0.87%)\t-  Highest reachable Recall : 0.6371\n",
    "- carts \t- Found 293.55K GTs with 128.21M candidates (pos_prop=0.23%)\t-  Highest reachable Recall : 0.5096\n",
    "- orders \t- Found 222.12K GTs with 128.21M candidates (pos_prop=0.17%)\t-  Highest reachable Recall : 0.7091"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chris v9 + Theo v5**\n",
    "- clicks \t- Found 1.16M GTs with 206.7M candidates (pos_prop=0.56%)\t-  Highest reachable Recall : 0.6590\n",
    "- carts \t- Found 309.85K GTs with 206.7M candidates (pos_prop=0.15%)\t-  Highest reachable Recall : 0.5379\n",
    "- orders \t- Found 227.4K GTs with 206.7M candidates (pos_prop=0.11%)\t-  Highest reachable Recall : 0.7260"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chris v8 + Theo v5**\n",
    "- clicks \t- Found 1.13M GTs with 167.12M candidates (pos_prop=0.68%)\t-  Highest reachable Recall : 0.6443\n",
    "- carts \t- Found 303.47K GTs with 167.12M candidates (pos_prop=0.18%)\t-  Highest reachable Recall : 0.5268\n",
    "- orders \t- Found 225.46K GTs with 167.12M candidates (pos_prop=0.13%)\t-  Highest reachable Recall : 0.7198"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chris v7 clicks + orders + Theo v5**\n",
    "- clicks \t- Found 1.14M GTs with 151.14M candidates (pos_prop=0.75%)\t-  Highest reachable Recall : 0.6473\n",
    "- carts \t- Found 301.08K GTs with 151.14M candidates (pos_prop=0.20%)\t-  Highest reachable Recall : 0.5227\n",
    "- orders \t- Found 224.5K GTs with 151.14M candidates (pos_prop=0.15%)\t-  Highest reachable Recall : 0.7167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chris v7 + Theo v5**\n",
    "- clicks \t- Found 1.09M GTs with 129.87M candidates (pos_prop=0.84%)\t-  Highest reachable Recall : 0.6224\n",
    "- carts \t- Found 295.05K GTs with 129.87M candidates (pos_prop=0.23%)\t-  Highest reachable Recall : **0.5122**\n",
    "- orders \t- Found 222.88K GTs with 129.87M candidates (pos_prop=0.17%)\t-  Highest reachable Recall : **0.7115**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clicks Chris v3 + Theo v5**\n",
    "- clicks \t- Found 1.12M GTs with 129M candidates (pos_prop=0.87%)\t-  Highest reachable Recall : **0.6361**\n",
    "- carts \t- Found 293.98K GTs with 129M candidates (pos_prop=0.23%)\t-  Highest reachable Recall : **0.5104**\n",
    "- orders \t- Found 222.29K GTs with 129M candidates (pos_prop=0.17%)\t-  Highest reachable Recall : **0.7097**\n",
    "\n",
    "**Chris v3 + Theo v5**\n",
    "- clicks\t- Found 1.11M GTs with 128.42M candidates (pos_prop=0.86%)\t-  Highest reachable Recall : 0.6298\n",
    "- carts\t- Found 292.58K GTs with 128.42M candidates (pos_prop=0.23%)\t-  Highest reachable Recall : 0.5079\n",
    "- orders\t- Found 222.05K GTs with 128.42M candidates (pos_prop=0.17%)\t-  Highest reachable Recall : 0.7089\n",
    "\n",
    "**Chris v4 + Theo v5**\n",
    "- clicks \t- Found 1.14M GTs with 161.5M candidates (pos_prop=0.70%)\t-  Highest reachable Recall : 0.6471\n",
    "- carts \t- Found 299.54K GTs with 161.5M candidates (pos_prop=0.19%)\t-  Highest reachable Recall : 0.5200\n",
    "- orders \t- Found 224.13K GTs with 161.5M candidates (pos_prop=0.14%)\t-  Highest reachable Recall : 0.7155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-otto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
