{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4474043,"sourceType":"datasetVersion","datasetId":2601572},{"sourceId":8603138,"sourceType":"datasetVersion","datasetId":5147566}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch-scatter    \n!pip install torch-sparse      \n!pip install torch-cluster     \n!pip install torch-spline-conv\n!pip install -q torch-geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:51:10.507316Z","iopub.execute_input":"2024-12-08T09:51:10.507948Z","iopub.status.idle":"2024-12-08T09:51:20.64087Z","shell.execute_reply.started":"2024-12-08T09:51:10.507903Z","shell.execute_reply":"2024-12-08T09:51:20.639725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd, numpy as np\nfrom itertools import product\nimport io, os, json\nimport tqdm\n\nimport time\n\nfrom sklearn.metrics import mean_squared_error\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\n\nfrom torch_geometric.nn import to_hetero\nfrom torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, GATConv, Linear, GraphConv\nfrom torch_geometric.data import Data, HeteroData\nfrom torch_geometric.loader import HGTLoader, NeighborLoader, LinkNeighborLoader\n\nimport polars as pl\n\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:02:31.053293Z","iopub.execute_input":"2024-12-08T10:02:31.054078Z","iopub.status.idle":"2024-12-08T10:02:39.38033Z","shell.execute_reply.started":"2024-12-08T10:02:31.05404Z","shell.execute_reply":"2024-12-08T10:02:39.379378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pl.read_parquet('/kaggle/input/otto-full-optimized-memory-footprint/train.parquet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:06:37.140622Z","iopub.execute_input":"2024-12-08T10:06:37.141623Z","iopub.status.idle":"2024-12-08T10:06:39.458396Z","shell.execute_reply.started":"2024-12-08T10:06:37.141572Z","shell.execute_reply":"2024-12-08T10:06:39.457648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:06:40.101491Z","iopub.execute_input":"2024-12-08T10:06:40.101856Z","iopub.status.idle":"2024-12-08T10:06:44.15419Z","shell.execute_reply.started":"2024-12-08T10:06:40.101825Z","shell.execute_reply":"2024-12-08T10:06:44.153441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.sample(frac=0.3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:06:45.312664Z","iopub.execute_input":"2024-12-08T10:06:45.313005Z","iopub.status.idle":"2024-12-08T10:07:02.421037Z","shell.execute_reply.started":"2024-12-08T10:06:45.312967Z","shell.execute_reply":"2024-12-08T10:07:02.42021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# session index dict\nsession = df['session'].unique()\nsource_idx = {id:idx for idx, id in enumerate(session)}\n\n# aid(article id) index dict\naid = df['aid'].unique()\ntarget_idx = {id:idx for idx, id in enumerate(aid)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:07:12.687531Z","iopub.execute_input":"2024-12-08T10:07:12.688405Z","iopub.status.idle":"2024-12-08T10:07:20.211146Z","shell.execute_reply.started":"2024-12-08T10:07:12.68837Z","shell.execute_reply":"2024-12-08T10:07:20.210362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"connected = df[['session', 'aid']]\nconnected['session'] = connected['session'].map(source_idx)\nconnected['aid'] = connected['aid'].map(target_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:07:26.555439Z","iopub.execute_input":"2024-12-08T10:07:26.555797Z","iopub.status.idle":"2024-12-08T10:08:34.281211Z","shell.execute_reply.started":"2024-12-08T10:07:26.555769Z","shell.execute_reply":"2024-12-08T10:08:34.280219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"source = connected['session']\ntarget = connected['aid']\nedge_index = torch.tensor((source.values, target.values))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:08:39.086856Z","iopub.execute_input":"2024-12-08T10:08:39.087461Z","iopub.status.idle":"2024-12-08T10:09:02.810725Z","shell.execute_reply.started":"2024-12-08T10:08:39.087424Z","shell.execute_reply":"2024-12-08T10:09:02.809648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Nodes Atrributes\nsession_num_nodes = df['session'].nunique()\naid_num_nodes = df['aid'].nunique()\nsession_id = torch.tensor(list(source_idx.values()), dtype=torch.int64)\n\naid_features = torch.rand((aid_num_nodes, 32)) # Create (random) article features with shape [num_node_aid, dimensions]\naid_id = torch.tensor(list(target_idx.values()), dtype=torch.int64)\n#aid_features = torch.nn.Embedding(aid_num_nodes, 32)\n\n## Edges Atrributes\nedge_index = edge_index\nedge_label = torch.tensor(df['type'].values).type(torch.int64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:09:21.607901Z","iopub.execute_input":"2024-12-08T10:09:21.608303Z","iopub.status.idle":"2024-12-08T10:09:26.843673Z","shell.execute_reply.started":"2024-12-08T10:09:21.608273Z","shell.execute_reply":"2024-12-08T10:09:26.842846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ndel df\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:09:35.792794Z","iopub.execute_input":"2024-12-08T10:09:35.793249Z","iopub.status.idle":"2024-12-08T10:09:35.978036Z","shell.execute_reply.started":"2024-12-08T10:09:35.793213Z","shell.execute_reply":"2024-12-08T10:09:35.977061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"node_types = {\n    'session': {\n        #'num_nodes': session_num_nodes,\n        'node_id': session_id\n    },\n    'aid': {\n        'x': aid_features,\n        'node_id': aid_id\n    }\n}\n\nedge_types = {\n    ('session', 'event', 'aid'): {\n        'edge_index': edge_index,\n        'edge_label': edge_label\n    }#,\n    #('session', 'cart', 'aid'): {\n        \n    #},\n    #('session', 'buy', 'aid'): {\n        \n    #}\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:09:41.327643Z","iopub.execute_input":"2024-12-08T10:09:41.328521Z","iopub.status.idle":"2024-12-08T10:09:41.333055Z","shell.execute_reply.started":"2024-12-08T10:09:41.328482Z","shell.execute_reply":"2024-12-08T10:09:41.331929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = HeteroData({**node_types, **edge_types})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:09:44.096593Z","iopub.execute_input":"2024-12-08T10:09:44.097551Z","iopub.status.idle":"2024-12-08T10:09:44.101826Z","shell.execute_reply.started":"2024-12-08T10:09:44.097502Z","shell.execute_reply":"2024-12-08T10:09:44.10104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del source, target, edge_index, edge_label\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:09:45.899111Z","iopub.execute_input":"2024-12-08T10:09:45.899931Z","iopub.status.idle":"2024-12-08T10:09:46.08021Z","shell.execute_reply.started":"2024-12-08T10:09:45.899895Z","shell.execute_reply":"2024-12-08T10:09:46.079216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:09:47.586466Z","iopub.execute_input":"2024-12-08T10:09:47.586807Z","iopub.status.idle":"2024-12-08T10:09:47.591228Z","shell.execute_reply.started":"2024-12-08T10:09:47.586778Z","shell.execute_reply":"2024-12-08T10:09:47.590276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# add sesion features for message passing:\ndata['session'].x = torch.rand(data['session'].num_nodes, 32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:09:48.652845Z","iopub.execute_input":"2024-12-08T10:09:48.653751Z","iopub.status.idle":"2024-12-08T10:09:51.476309Z","shell.execute_reply.started":"2024-12-08T10:09:48.653711Z","shell.execute_reply":"2024-12-08T10:09:51.475306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add a reverse ('movie', 'rev_rates', 'user') relation for message passing:\ndata = T.ToUndirected()(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:09:55.229272Z","iopub.execute_input":"2024-12-08T10:09:55.229923Z","iopub.status.idle":"2024-12-08T10:09:55.676369Z","shell.execute_reply.started":"2024-12-08T10:09:55.229889Z","shell.execute_reply":"2024-12-08T10:09:55.675526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del data['aid', 'rev_event', 'session'].edge_label  # Remove \"reverse\" label.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:10:19.007837Z","iopub.execute_input":"2024-12-08T10:10:19.008909Z","iopub.status.idle":"2024-12-08T10:10:19.012696Z","shell.execute_reply.started":"2024-12-08T10:10:19.00887Z","shell.execute_reply":"2024-12-08T10:10:19.011923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del data['session'].num_nodes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:10:21.616799Z","iopub.execute_input":"2024-12-08T10:10:21.61722Z","iopub.status.idle":"2024-12-08T10:10:21.621478Z","shell.execute_reply.started":"2024-12-08T10:10:21.617187Z","shell.execute_reply":"2024-12-08T10:10:21.62052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:10:22.463193Z","iopub.execute_input":"2024-12-08T10:10:22.463912Z","iopub.status.idle":"2024-12-08T10:10:22.469748Z","shell.execute_reply.started":"2024-12-08T10:10:22.463877Z","shell.execute_reply":"2024-12-08T10:10:22.468577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform a link-level split into training, validation, and test edges:\ntrain_data, val_data, test_data = T.RandomLinkSplit(\n    num_val=0.1,\n    num_test=0.1,\n    neg_sampling_ratio=0.0,\n    edge_types=[('session', 'event', 'aid')],\n    rev_edge_types=[('aid', 'rev_event', 'session')],\n)(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:10:23.629015Z","iopub.execute_input":"2024-12-08T10:10:23.629375Z","iopub.status.idle":"2024-12-08T10:10:46.73504Z","shell.execute_reply.started":"2024-12-08T10:10:23.629344Z","shell.execute_reply":"2024-12-08T10:10:46.734027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define seed edges:\nedge_label_index = train_data['session', 'event', 'aid'].edge_label_index\nedge_label = train_data['session', 'event', 'aid'].edge_label\n\ntrain_loader = LinkNeighborLoader(\n    data=train_data,  # TODO\n    num_neighbors=[3, 1],  # TODO\n    neg_sampling_ratio=0.0,  # TODO\n    edge_label_index=(('session', 'event', 'aid'), edge_label_index),\n    edge_label=edge_label,\n    batch_size=128,\n    shuffle=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:21:51.3848Z","iopub.execute_input":"2024-12-08T10:21:51.385841Z","iopub.status.idle":"2024-12-08T10:22:06.95958Z","shell.execute_reply.started":"2024-12-08T10:21:51.385785Z","shell.execute_reply":"2024-12-08T10:22:06.958354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#We have an unbalanced dataset with many labels for rating 3 and 4, and very\n# few for 0 and 1. Therefore we use a weighted MSE loss.\nweight = torch.bincount(train_data['session', 'aid'].edge_label)\nweight = weight.max() / weight","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:23:21.123665Z","iopub.execute_input":"2024-12-08T10:23:21.124809Z","iopub.status.idle":"2024-12-08T10:23:21.261007Z","shell.execute_reply.started":"2024-12-08T10:23:21.124766Z","shell.execute_reply":"2024-12-08T10:23:21.260201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef weighted_mse_loss(pred, target, weight=None):\n    weight = 1. if weight is None else weight[target].to(pred.dtype)\n    return (weight * (pred - target.to(pred.dtype)).pow(2)).mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:23:24.433563Z","iopub.execute_input":"2024-12-08T10:23:24.43428Z","iopub.status.idle":"2024-12-08T10:23:24.438653Z","shell.execute_reply.started":"2024-12-08T10:23:24.434246Z","shell.execute_reply":"2024-12-08T10:23:24.437747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GNNEncoder(torch.nn.Module):\n    def __init__(self, hidden_channels, out_channels):\n        super().__init__()\n        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n        self.conv2 = SAGEConv((-1, -1), out_channels)\n\n    def forward(self, x, edge_index):\n        x = self.conv1(x, edge_index).relu()\n        x = self.conv2(x, edge_index)\n        return x\n\n\nclass EdgeDecoder(torch.nn.Module):\n    def __init__(self, hidden_channels):\n        super().__init__()\n        self.lin1 = Linear(2 * hidden_channels, hidden_channels)\n        self.lin2 = Linear(hidden_channels, 1)\n\n    def forward(self, z_dict, edge_label_index):\n        row, col = edge_label_index\n        z = torch.cat([z_dict['session'][row], z_dict['aid'][col]], dim=-1)\n\n        z = self.lin1(z).relu()\n        z = self.lin2(z)\n\n        return z.view(-1)\n\n\nclass Model(torch.nn.Module):\n    def __init__(self, hidden_channels):\n        super().__init__()\n        # encoder and decoder\n        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n        self.decoder = EdgeDecoder(hidden_channels)\n\n        # embedding matrices for sessions and aids\n        #self.aid_lin = Linear(hidden_channels, hidden_channels)\n        #self.session_emb = torch.nn.Embedding(data['session'].num_nodes, hidden_channels)\n        #self.aid_emb = torch.nn.Embedding(data['aid'].num_nodes, hidden_channels)\n\n    def forward(self, x_dict, edge_index_dict, edge_label_index):\n        #x_dict = {\n        #    'session': self.session_emb(data['session'].node_id.to(device)),\n        #    'aid': self.aid_lin(data['aid'].x.to(device).float()) + self.aid_emb(data['aid'].node_id.to(device))\n        #}\n        z_dict = self.encoder(x_dict, edge_index_dict)\n        return self.decoder(z_dict, edge_label_index)\n\n    def get_embedding(self, x_dict, edge_index_dict):\n        return self.encoder(x_dict, edge_index_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:23:32.570806Z","iopub.execute_input":"2024-12-08T10:23:32.571686Z","iopub.status.idle":"2024-12-08T10:23:32.580551Z","shell.execute_reply.started":"2024-12-08T10:23:32.571646Z","shell.execute_reply":"2024-12-08T10:23:32.579636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Model(hidden_channels=32).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:23:33.286692Z","iopub.execute_input":"2024-12-08T10:23:33.287562Z","iopub.status.idle":"2024-12-08T10:23:33.957541Z","shell.execute_reply.started":"2024-12-08T10:23:33.287523Z","shell.execute_reply":"2024-12-08T10:23:33.956799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sampled_data = next(iter(train_loader))\n\nwith torch.no_grad():\n    model.encoder(sampled_data.to(device).x_dict, sampled_data.to(device).edge_index_dict)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T10:23:36.710866Z","iopub.execute_input":"2024-12-08T10:23:36.711233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(train_data=sampled_data):\n    model.train()\n    optimizer.zero_grad()\n    pred = model(train_data.x_dict, train_data.edge_index_dict,\n                 train_data[('session', 'event', 'aid')].edge_label_index)\n    target = train_data['session', 'aid'].edge_label\n    loss = weighted_mse_loss(pred, target, weight.to(device))\n    #loss = F.binary_cross_entropy_with_logits(pred, target)\n\n    # Embedding\n    emb_dict = model.get_embedding(train_data.x_dict, train_data.edge_index_dict)\n\n    loss.backward()\n    optimizer.step()\n    return float(loss), pred, model, emb_dict\n\n@torch.no_grad()\ndef test(data=data):\n    model.eval()\n    #pred = model(data.x_dict, data.edge_index_dict,\n    #             data['session', 'aid'].edge_label_index)\n    pred = model(data.edge_index_dict,\n                 data['session', 'aid'].edge_label_index)\n    pred = pred.clamp(min=0, max=2)\n    target = data['session', 'aid'].edge_label.float()\n    rmse = F.mse_loss(pred, target).sqrt()\n    return float(rmse), pred, target","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nimport torch.nn.functional as F\n\n#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: '{device}'\")\n\n#model = Model(hidden_channels=32).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nfor epoch in range(0, 1):\n    total_loss = total_examples = 0\n    for sampled_data in tqdm.tqdm(train_loader):\n        sampled_data = sampled_data.to(device)\n        optimizer.zero_grad()\n        loss, pred, model, emb_dict = train(train_data=sampled_data)\n        #loss, pred, model = train(model=model, train_data=sampled_data)\n        # TODO: Move `sampled_data` to the respective `device`\n        # TODO: Run `forward` pass of the model\n        # TODO: Apply binary cross entropy via\n        # `F.binary_cross_entropy_with_logits(pred, ground_truth)`\n\n        #loss.backward()\n        #optimizer.step()\n        total_loss += float(loss) * pred.numel()\n        total_examples += pred.numel()\n    print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path = '/kaggle/working/model.pt'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the entire model\ntorch.save(model, path)\nmodel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Load the entire model\nmodel = torch.load(path, map_location=torch.device('cpu'))\nmodel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(path,mode=\"validation\",n_neighbors=20):\n\n\n    test = pl.read_parquet(path)\n\n    session_types = ['clicks', 'carts', 'orders']\n\n    test_session_AIDs = test.to_pandas().reset_index(drop=True).groupby('session')['aid'].apply(list)\n\n    test_session_types = test.to_pandas().reset_index(drop=True).groupby('session')['type'].apply(list)\n\n    del test\n    gc.collect()\n\n    labels = []\n\n    type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n\n    for AIDs, types in zip(test_session_AIDs, test_session_types):\n        # AIDs: list of connected aids in each session\n        # types: list of connected types in each session\n        if len(AIDs) >= 20: # this session have at least 20 aids connected\n            # if we have enough aids (over equals 20) we don't need to look for candidates! we just use the old logic\n            weights = np.logspace(0.1,1,len(AIDs), base=2, endpoint=True) - 1\n            aids_temp = defaultdict(lambda: 0)\n            for aid, w, t in zip(AIDs, weights, types): \n                # {aid : weight}\n                aids_temp[aid] += w * type_weight_multipliers[t]\n\n            # list of sorted dictionary key by value\n            sorted_aids = [k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]\n            labels.append(sorted_aids[:20]) # only top 20\n        else:\n            # here we don't have 20 aids to output -- we will use word2vec embeddings to generate candidates!\n            AIDs = list(dict.fromkeys(AIDs[::-1]))\n\n            # let's grab the most recent aid\n            most_recent_aid = AIDs[0]\n\n            # and look for some neighbors!\n            nns = [i for i in index.get_nns_by_item(most_recent_aid, n_neighbors+1)[1:]]\n\n\n            labels.append((AIDs+nns)[:n_neighbors])\n\n    labels_as_strings = [' '.join([str(l) for l in lls]) for lls in labels]\n\n    predictions = pd.DataFrame(data={'session_type': test_session_AIDs.index, 'labels': labels_as_strings})\n\n    prediction_dfs = []\n\n    for st in session_types:\n        modified_predictions = predictions.copy()\n        modified_predictions.session_type = modified_predictions.session_type.astype('str') + f'_{st}'\n        prediction_dfs.append(modified_predictions)\n\n    sub = pd.concat(prediction_dfs).reset_index(drop=True)\n    \n    del prediction_dfs, predictions,labels_as_strings, labels, test_session_types,test_session_AIDs\n    gc.collect()\n    if mode==\"test\":\n        sub.to_csv(\"submission.csv\",index=False)\n        return sub\n    else:\n        sub['labels_2'] = sub['labels'].apply(lambda x : [int(s) for s in x.split(' ')])\n        submission = pd.DataFrame()\n        submission['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n        submission['type'] = sub.session_type.apply(lambda x: x.split('_')[1])\n        submission['labels'] = sub.labels_2.apply(lambda x : [item for item in x[:] ]) #.apply(lambda x: [int(i) for i in x.split(',')[:20]])\n        test_labels = pd.read_parquet('/content/drive/MyDrive/OTTO-Kaggle-Competition/for-local-validation/test_labels.parquet')\n        test_labels = test_labels.merge(submission, how='left', on=['session', 'type'])\n        del sub,submission\n        gc.collect()\n        gc.collect()\n        test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n        test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n        recall_per_type = test_labels.groupby(['type'])['hits'].sum() / test_labels.groupby(['type'])['gt_count'].sum() \n        score = (recall_per_type * pd.Series({'clicks': 0.1, 'carts': 0.30, 'orders': 0.60})).sum()\n\n        return score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path = \"/kaggle/input/otto-full-optimized-memory-footprint/test.parquet\"\ntest_submission = evaluate(path,mode=\"test\",n_neighbors=20)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}